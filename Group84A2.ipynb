{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRBLkUPE7Aet",
        "outputId": "3dfee81b-12a8-43b1-89c0-7a1215cf9d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# load packages\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import spacy\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. load data\n"
      ],
      "metadata": {
        "id": "_E8bjOsfaakP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tEba8ZR8LozB"
      },
      "outputs": [],
      "source": [
        "# load datasets: since only focused on report, so train.csv and val.csv were used\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cRd9FYPOPzvF"
      },
      "outputs": [],
      "source": [
        "id = '1zTa79IzS1uSW0wVILnVHsVx97xNbCb88'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.csv')  \n",
        "# https://drive.google.com/file/d/1zTa79IzS1uSW0wVILnVHsVx97xNbCb88/view?usp=sharing #train\n",
        "# https://drive.google.com/file/d/1913G0dNuYpWh0v-Fh2RUoq1piMfLTIOa/view?usp=sharing #vali\n",
        "# https://drive.google.com/file/d/1ysKwXTo2J2w4Vd9hALSJNhMSIEIQv2X6/view?usp=sharing #test\n",
        "# https://drive.google.com/file/d/1-KpmplTKfKfoAo8v6b-urrwF6iHQomzp/view?usp=sharing. #sample\n",
        "id = '1913G0dNuYpWh0v-Fh2RUoq1piMfLTIOa'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('val.csv')  \n",
        "id = '1ysKwXTo2J2w4Vd9hALSJNhMSIEIQv2X6'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('test_without_labels.csv') \n",
        "id = '1-KpmplTKfKfoAo8v6b-urrwF6iHQomzp'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('sample.csv')\n",
        "\n",
        "#Reference: COMP5046, Lab05,https://colab.research.google.com/drive/1qgMqtdKGy9geQ4IUx3pd8PovUQLFswW8#scrollTo=vh1rYL3NIavi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2SPjEtpILo1i",
        "outputId": "69a2a25d-2cc4-4bed-f701-a80b92edb585"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sents labels\n",
              "0      wow      O\n",
              "1      WTF      T\n",
              "2  wpe wpe    O O\n",
              "3   hahaha      O\n",
              "4      wtf      T"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d8af7da-1415-40cb-aa9c-25c6b72a3c65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WTF</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wpe wpe</td>\n",
              "      <td>O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hahaha</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wtf</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d8af7da-1415-40cb-aa9c-25c6b72a3c65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9d8af7da-1415-40cb-aa9c-25c6b72a3c65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9d8af7da-1415-40cb-aa9c-25c6b72a3c65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#load data\n",
        "trainset=pd.read_csv('train.csv')\n",
        "valset=pd.read_csv('val.csv')\n",
        "testset=pd.read_csv('test_without_labels.csv')\n",
        "trainset.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "cuYCznlONro9",
        "outputId": "4be82ade-a92b-4200-c343-51193c2c66fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 sents\n",
              "0                                               FUCKER\n",
              "1                                                hahha\n",
              "2                                                ggggg\n",
              "3                                            macropyre\n",
              "4                                                 Boom\n",
              "..                                                 ...\n",
              "495  REPORT SPECTRE RAGE QUIT [SEPA] AND HIM FRIEND...\n",
              "496                                sf feeder auto lose\n",
              "497                         GG [SEPA] COMMEND SUPPORTS\n",
              "498     how [SEPA] WTF [SEPA] THIS IS NOT EVEN REALITY\n",
              "499           coming edi wait awhile ya [SEPA] fuck me\n",
              "\n",
              "[500 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6c9d4b2-42d2-4211-ab53-0e97adff9045\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FUCKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hahha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ggggg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>macropyre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Boom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>REPORT SPECTRE RAGE QUIT [SEPA] AND HIM FRIEND...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>sf feeder auto lose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>GG [SEPA] COMMEND SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>how [SEPA] WTF [SEPA] THIS IS NOT EVEN REALITY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>coming edi wait awhile ya [SEPA] fuck me</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6c9d4b2-42d2-4211-ab53-0e97adff9045')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e6c9d4b2-42d2-4211-ab53-0e97adff9045 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e6c9d4b2-42d2-4211-ab53-0e97adff9045');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DBWozOaRRYzb"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_label=list(trainset.labels)\n",
        "val_label=list(valset.labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ctpqizFZUDv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences=list(trainset.sents)\n",
        "val_sentences=list(valset.sents)\n",
        "test_sentences = list(testset.sents)"
      ],
      "metadata": {
        "id": "LU_BohCzzyw4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1Data processing"
      ],
      "metadata": {
        "id": "82POApsWzJqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LQPPxX5pRohY"
      },
      "outputs": [],
      "source": [
        "# split all labels by space\n",
        "train_label_split=[]\n",
        "val_label_split=[]\n",
        "for i in train_label:\n",
        "    train_label_split.append(i.split(\" \"))\n",
        "for j in val_label:\n",
        "    val_label_split.append(j.split(\" \"))\n",
        "\n",
        "# Split all data parts by space\n",
        "tokenized_train=[]\n",
        "tokenized_val=[]\n",
        "tokenized_test=[]\n",
        "for i in train_sentences:\n",
        "    token=i.lower().split(\" \")\n",
        "    tokenized_train.append(token)\n",
        "for j in val_sentences:\n",
        "    token=j.lower().split(\" \")\n",
        "    tokenized_val.append(token)\n",
        "for k in test_sentences:\n",
        "    token=k.lower().split(\" \")\n",
        "    tokenized_test.append(token)\n",
        "#token all the train,validation and test dataset and lower it's capital\n",
        "\n",
        "#reference: comp5046,lab02,https://colab.research.google.com/drive/1-l7gzLZ71ERuJ_ktG1nKTU6G8UuEvseN?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "concate_tokens=tokenized_train+tokenized_val+tokenized_test"
      ],
      "metadata": {
        "id": "yGXZ-AQLPRLR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UodogVmSNXh",
        "outputId": "78ea9a70-9dac-4195-c0a7-e5bf24df8b22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26078"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFXkQctdSL0m",
        "outputId": "3a6f7012-2051-455a-e24e-c1d44941b7b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8705"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y0X7x6ewNuk",
        "outputId": "3886355b-07f2-4324-d8b8-f4f92a753b8f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Input embedding"
      ],
      "metadata": {
        "id": "_BiL7nsP06Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 word embedding"
      ],
      "metadata": {
        "id": "I0hBOk6i1Din"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fb438MxZ1OqX"
      },
      "outputs": [],
      "source": [
        "#make a sentence dictionary\n",
        "# Generate word_to_ix\n",
        "word_to_ix = {}\n",
        "for sentence in concate_tokens: #all tokens\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "word_list = list(word_to_ix.keys()) # get all keys of tokens\n",
        "\n",
        "# Generate tag_to_ix\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "for tags in train_label_split + val_label_split: #train and valid lables\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_to_ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD0S70Ya19G2",
        "outputId": "5d716e15-98d1-4391-9e3c-26153dff9c11"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<START>': 0,\n",
              " '<STOP>': 1,\n",
              " 'C': 8,\n",
              " 'D': 7,\n",
              " 'O': 2,\n",
              " 'P': 4,\n",
              " 'S': 6,\n",
              " 'SEPA': 5,\n",
              " 'T': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for output later\n",
        "ix_to_tag = dict(zip([i for i in tag_to_ix.values()],[i for i in tag_to_ix.keys()]))"
      ],
      "metadata": {
        "id": "lYJqX4Mb2SIy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ix_to_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj1_PY78238Y",
        "outputId": "4d1cafbe-a6fd-4aa2-8a8f-8a298264f011"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '<START>',\n",
              " 1: '<STOP>',\n",
              " 2: 'O',\n",
              " 3: 'T',\n",
              " 4: 'P',\n",
              " 5: 'SEPA',\n",
              " 6: 'S',\n",
              " 7: 'D',\n",
              " 8: 'C'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset to idxs\n",
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "          input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "train_input_index =  to_index(tokenized_train,word_to_ix)\n",
        "train_output_index = to_index(train_label_split,tag_to_ix)\n",
        "val_input_index = to_index(tokenized_val,word_to_ix)\n",
        "val_output_index = to_index(val_label_split,tag_to_ix)\n",
        "test_input_index = to_index(tokenized_test,word_to_ix)\n",
        "#test have no label\n",
        "\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "id": "eCz2f_rYLP-I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 POS tag & parse"
      ],
      "metadata": {
        "id": "zhy_ems62_iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reference: comp5046,lab07, https://colab.research.google.com/drive/1vfhLu945Wi00WPhWAMBVQe4RbUIdqH05?usp=sharing#scrollTo=rFBFgEEkIaQ-"
      ],
      "metadata": {
        "id": "PYaEh-q7TxxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pszXM3qM5d2n"
      },
      "outputs": [],
      "source": [
        "# Get all pos tags\n",
        "def pos_tag(doc):\n",
        "    pos=[]\n",
        "    for i in doc:\n",
        "        tags=[]\n",
        "        for w,t in nltk.pos_tag(i):\n",
        "            tags.append(t)\n",
        "        pos.append(tags)\n",
        "    return pos\n",
        "\n",
        "pos_tags=pos_tag(concate_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "j0qvCvb96MEF"
      },
      "outputs": [],
      "source": [
        "# use Word2Vec:SkigGram to train pos tags\n",
        "word_to_pos=Word2Vec(sentences=pos_tags,size=20,window=3,min_count=1, workers=4, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HSJ1tYQM61Ok"
      },
      "outputs": [],
      "source": [
        "# Get all parse dependency\n",
        "#load the spacy api with the pre-trained statistical models for English. English multi-task CNN trained on OntoNotes\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "def parse_depend(word):\n",
        "    parse_sent=[]\n",
        "    for i in word:\n",
        "        parse=nlp(' '.join(i))\n",
        "        x=[]\n",
        "        for j in parse:\n",
        "            x.append(j.dep_)            \n",
        "        parse_sent.append(x[:len(i)])\n",
        "    return parse_sent\n",
        "\n",
        "parse_sents=parse_depend(concate_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "64vXA_527pQO"
      },
      "outputs": [],
      "source": [
        "# use Word2Vec:SkipGram to train all parses \n",
        "word_to_parse=Word2Vec(sentences=parse_sents,size=20,window=3,min_count=1,workers=4,sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpaOcljP7u3U",
        "outputId": "f91f477a-47f0-4666-c361-628cdb74e932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ],
      "source": [
        "#Create dictionaries to store all pos tags and parse\n",
        "word_2_pos={}\n",
        "for i in range(0,len(concate_tokens)):\n",
        "    for x in range(0,len(concate_tokens[i])):\n",
        "        word_2_pos[concate_tokens[i][x]]=word_to_pos[pos_tags[i][x]]\n",
        "word_2_parse={} \n",
        "for i in range(0,len(concate_tokens)):\n",
        "    for x in range(0,len(concate_tokens[i])):\n",
        "        word_2_parse[concate_tokens[i][x]]=word_to_parse[parse_sents[i][x]]\n",
        "\n",
        "#reference: comp5046,lab07, https://colab.research.google.com/drive/1vfhLu945Wi00WPhWAMBVQe4RbUIdqH05?usp=sharing#scrollTo=rFBFgEEkIaQ-"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 domian & enbedding matrix "
      ],
      "metadata": {
        "id": "F6vkTGg975lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_dota =\"\"\"AA \n",
        "Initialism for Ancient Apparition.\n",
        "AC \n",
        "Initialism for Assault Cuirass.\n",
        "Aggro \n",
        "Abbreviation for aggression. The programmed aggression of AI controlled towers and creeps. Refers to getting the attention of a particular hostile unit, e.g. \"I have aggro\" means a hostile unit is focusing on attacking you.\n",
        "AM \n",
        "Initialism for Anti-Mage.\n",
        "AoE \n",
        "Initialism for Area of Effect. It refers to a spell, attack, or effect that affects an area around a point (for example, Dragon Slave is an \"AoE spell\"). Also used to refer to the range or size of one such spell/attack/effect, for example Reverse Polarity affects enemies in a 410 radius, therefore it has an AoE of 410, or \"410 AoE\".\n",
        "AW \n",
        "Initialism for Arc Warden.\n",
        "B \n",
        "Initialism for \"Back\" or \"Get Back\". Used to call for a retreat. A player saying \"b\" is usually suggesting that everyone back up or run away.\n",
        "BAT \n",
        "Abbreviation for Base Attack Time, which determines how long an unbuffed unit with 0 agility and 0% bonus attack speed has to wait to attack again. Lower BAT units are able to attack more frequently. Most heroes have a BAT of 1.70. It is considered better to have a lower BAT and worse to have a higher BAT.\n",
        "Backdoor \n",
        "The act of attacking enemy structures while a player's own lane creeps have not reached the structure yet. Usually ineffective due to backdoor protection.\n",
        "Ball \n",
        "May refer to Io or snowballing or close positioning of teammates when pushing highground ('don't ball for ravage')\n",
        "Bara, Barathum \n",
        "Refers to Spirit Breaker's name as he was commonly referenced in Dota 1.\n",
        "Barracks\n",
        "Structures found in each lane that when destroyed permanently grant super creeps to the team that destroyed it.\n",
        "Basi \n",
        "refers to Ring of Basilius\n",
        "BB \n",
        "Initialism for Bristleback or Buyback.\n",
        "BD \n",
        "Initialism for backdooring.\n",
        "BF \n",
        "Initialism for Battle Fury, also often referred to as \"bfury\", or Butterfly.\n",
        "BH \n",
        "Initialism for either Bounty Hunter, or Enigma's ultimate, Black Hole.\n",
        "Bird \n",
        "May refer to Phoenix, Skywrath Mage (and his spells), Visage's Familiars, or the flying courier.\n",
        "BKB \n",
        "Initialism for Black King Bar.\n",
        "Blocking \n",
        "Refers to one of the following: the act of \"blocking\" creeps with your hero's body as they run down the lane, impeding their movement so that the creeps meet closer toward your tower; the act of deliberately and repeatedly moving directly in front of an enemy hero (in the same manner as creep blocking) to impede their movement, often to help allies attack them; the act of \"blocking\" a neutral camp with a unit or a ward -- because if there is a unit or ward in the spawn box at X:00 on the clock, the camp will not spawn new creeps.\n",
        "BM \n",
        "Initialism for Beastmaster, Brewmaster, Broodmother, Blade Mail or \"bad manners\".\n",
        "Bot \n",
        "Refers to either the bottom lane of the map, or an AI-controlled hero.\n",
        "BoT, BoTs\n",
        "Initialism for Boots of Travel.\n",
        "Book \n",
        "Refers to Necronomicon, short for \"necro book\". Can also be referred to as book1, book2, or book3 based on its level.\n",
        "Bottle Crow \n",
        "The tactic of giving one's Bottle to the courier, then sending it on a round trip back to base to refill the bottle and give it back to the owner. This keeps the player's HP/MP healthy, and they do not have to miss any exp/gold from lane refilling it themselves. Note that any bottle that is not full slows the courier by 30%, and the act of bottle crowing may hamper player's allies as it gets harder to deliver their own items while the courier is making trips. (no longer possible, patched out)\n",
        "BRB \n",
        "Initialism for \"be right back\".\n",
        "Break \n",
        "Refers to either the effect of disabling the effects of a hero's passive abilities or the other passive ability of Tranquil Boots.\n",
        "BS \n",
        "Initialism for Bloodseeker or Bloodstone. May also be used as an acronym for \"bullshit\".\n",
        "Buff \n",
        "A beneficial spell or effect placed on a unit. Refers to the opposite of a debuff.\n",
        "Burst or Burst Damage \n",
        "Refers to a high amount of damage dealt over a short period of time. Usually involves nukes.\n",
        "Carry \n",
        "A type of hero which can overpower the enemy team in the late game. These heroes tend to scale very well with gold and experience, and thus require large sums of it in order to be successful.\n",
        "Caster \n",
        "Either refers to a hero whose primary function is to cast spells prolifically, or a live person who commentates on an ongoing game.\n",
        "CC \n",
        "Initialism for Crowd Control. See disable.\n",
        "CD \n",
        "Initialism for cooldown or Captain's Draft.\n",
        "CDR \n",
        "Initialism for cooldown reduction.\n",
        "Chicken or Chick \n",
        "Refers to Animal Courier.\n",
        "Chieftain \n",
        "See Tauren Chieftain.\n",
        "CK \n",
        "Initialism for Chaos Knight. May also refer to \"creep kill\" (see CS or last hit).\n",
        "CM \n",
        "Initialism for Crystal Maiden or Captain's Mode.\n",
        "Comeback \n",
        "A situation when a team is performing poorly for most of the game, but has managed to fight back, turn the tide of battle and ultimately win the game.\n",
        "Crow \n",
        "Refers to Flying Courier, which upgrades a walking courier. A player asking to \"please crow it\" is asking for someone to upgrade the courier.\n",
        "CS \n",
        "Initialism for Creep Score, the amount of last hits a player has, or the amount of last hits and denies a player has. In DotA 1, typing -cs would show the player's last hits and denies count. May also refer to \"creep steal\", the act of stealing last hits from an allied player.\n",
        "CW \n",
        "Initialism for Centaur Warrunner or Clockwerk.\n",
        "DD \n",
        "Initialism for Rune of Double Damage.\n",
        "Debuff \n",
        "A detrimental spell of effect placed on a unit. Refers to the opposite of a buff.\n",
        "Deny \n",
        "Killing your allied unit, hero or building, in order to prevent an enemy from gaining the gold and experience it gives. You can manually attack your own creeps if their health is below 50%, your own towers below if they are below 10%, and heroes affected by certain DoTs. A hero can also be said to be denied if they die to neutral creeps instead of to an enemy unit.\n",
        "Deward \n",
        "The act of removing enemy wards, usually mostly pertaining to Observer Wards.\n",
        "DGU \n",
        "Initialism for \"Don't give up\".\n",
        "Dieback \n",
        "When a player dies after using their Buyback, resulting in a much longer respawn time and them unable to use it once more as it will most definitely be on cooldown.\n",
        "Disable \n",
        "A catch-all term referring to any spell, ability, or effect that otherwise prevents an enemy hero from moving, casting, and/or attacking, leaving them helpless for a short period of time or in general impeding their ability to act. See the main article for more information.\n",
        "Dive \n",
        "The act of running at a hero that is within their tower range in order to attempt to secure a kill. This can sometimes be a risky play in the early stages of the game when tower damage output is more significant. Dives are usually attempted by more than one hero, but can also be done by a single hero. Also referred to as a \"tower dive\".\n",
        "DK \n",
        "Initialism for Dragon Knight.\n",
        "DoT \n",
        "Damage over Time, an applied effect that repeatedly inflicts damage at a regular interval for a specific duration. A classic example would be Venomancer's Poison Nova.\n",
        "DP \n",
        "Initialism for Death Prophet.\n",
        "DPS \n",
        "Initialism for damage per second, a measure of the damage dealt by a hero or unit over one second. Usually refers to heroes with consistent damage output over a period rather than heroes with high burst damage periodically.\n",
        "Dunk \n",
        "Refers to either Axe's ultimate, Culling Blade, or Earthshaker's ultimate, Echo Slam.\n",
        "Durable \n",
        "A hero that can take a lot of damage and abuse before dying. See the main article for more information.\n",
        "Dust \n",
        "Abbreviation for the item, Dust of Appearance.\n",
        "Easy lane\n",
        "See Safe Lane.\n",
        "Egg \n",
        "Refers to Phoenix' ultimate, Supernova.\n",
        "Epi \n",
        "Refers to Sand King's ultimate, Epicenter.\n",
        "ES \n",
        "Initialism for Earthshaker, Earth Spirit, or Ember Spirit.\n",
        "ET \n",
        "Initialism for Elder Titan.\n",
        "Exp or XP \n",
        "Experience, the resource used by heroes to gain levels, level skills, and otherwise increase in power over the course of a game.\n",
        "ez \n",
        "\"Easy\", typically said to mock the enemy team's lesser power.\n",
        "F - J\n",
        "Fade time \n",
        "The amount of time it takes for a unit to become completely invisible following the activating of an invisibility effect.\n",
        "Farm or Farming \n",
        "The process of steadily earning gold and experience by killing lane creeps and/or neutral creeps. This tactic is often slow and tedious, but is usually necessary for Carries to reach their full potential.\n",
        "FB \n",
        "Initialism for First Blood.\n",
        "Feed \n",
        "The act of \"feeding\" gold and experience to the enemy team by dying repeatedly. Can be intentional or unintentional. Those who feed are called \"feeders\".\n",
        "Flash Farming \n",
        "A style of farming when you farm your lane creeps until the enemy tower and then proceed to clear neutral creep camps in quick and efficient rotations. This is often the fastest method of farming, but it also allows the enemy to farm your creeps freely by their tower and can be risky, as being near the enemy tower is often an easy gank, or the enemy might know exactly where you are clearing the camp closest to the lane tanking creeps and losing health, etc.\n",
        "Flash Farming Skill/Ability\n",
        "An AoE ability that facilitates flash farming by allowing a hero to quickly clear lane creeps or groups of neutrals. The classic flash farming ability is Shadow Fiend's Shadowraze which allows him to mow down wave after wave of creeps with ease. It is common to hit lane creeps below threshold and then clear them all in one shot with the flash farming ability, and then go off to check rune, kill neutrals, etc.\n",
        "FF \n",
        "According to DotA history, \"ff\" appeared on DotA-League.com (at this time it was for DotA 1), and it means \"forfeit\". You had to type \"-ff\" in the game so it can detected and recorded by the dota-league plugin and save the data to your dota-league profile. If all 5 members of one team type it the game would end. Now replaced by calling \"gg\" if the surrender option is enabled in a lobby. More commonly used as abbreviation to \"finish fast\", asking enemy team to end the game that's already unwinnable for other side.\n",
        "Fortification/Fortify \n",
        "The Glyph of Fortification is the button in the bottom right hand of the screen that renders all allied buildings invulnerable for 5 seconds on a 5 minute cooldown. If a player says \"don't fort\", they are telling the team to save the glyph for later. Towers are \"fortified\" for the 5 seconds that the glyph is active.\n",
        "FoW \n",
        "Initialism for Fog of War. Refers to the portion of the map that is dark and unseen. If you cannot see an area, then it is said to be \"fogged\". Most of the map is \"fogged\" by default, such as Roshan's lair, and the enemy base.\n",
        "Furion \n",
        "Nature's Prophet's name in WC3 DotA. Still referenced in the game's files for the hero.\n",
        "Gank \n",
        "Abbreviation for Gang Kill, but over time the term has come to refer to any time a hero or group of heroes attempts to pick off an enemy hero (or enemy heroes) by surprise, but usually with superior forces (such as 2v1).\n",
        "Gem \n",
        "The Gem of True Sight. Most commonly means to either buy a gem, or to be careful of the enemy team as they possess a gem (especially for allied invisible heroes).\n",
        "gg \n",
        "\"Good game\". Said by either team when they are winning or losing and the match is close to ending, or when a team or a player gives up or claims victory. For example, both teams might say \"gg\" to congratulate each other after a hard fought match, or a team might call out \"gg\" after their ally abandons in an assumption that they can no longer win.\n",
        "ggwp \n",
        "\"Good game, well played\". Used in a similar fashion as \"gg\", but also congratulates the enemy for skillful play.\n",
        "gj \n",
        "\"Good job\", typically said to compliment an ally.\n",
        "glhf \n",
        "\"Good luck, have fun\". Usually said at the start of a game to encourage other players, or just to be courteous.\n",
        "Golem \n",
        "Refers to Mud Golems, Ancient Golems, or Warlock's ultimate summon through Chaotic Offering.\n",
        "Gs \n",
        "Initialism for Grimstroke.\n",
        "Guinsoo \n",
        "A reference to Scythe of Vyse's full name in WC3 DotA, which in turn references one of the developers of the original DotA maps.\n",
        "Hard Carry \n",
        "A type of carry hero which scales incredibly well with items and requires a substantial amount of farm to be effective. Examples of this are Anti-Mage and Spectre. Hard carries are usually weaker than other semi-carry or non-carry heroes at the early-mid stages of the game, when they do not yet have their core items.\n",
        "Hard Lane \n",
        "The lane of either faction where the creep wave meets up furthest from the tower, making those who lane in it more susceptible to ganks (top lane for Radiant, bottom lane for Dire). See Lane.\n",
        "HH \n",
        "Initialism for the item Heaven's Halberd.\n",
        "Hook \n",
        "Refers to either Pudge's signature ability, Meat Hook, or Clockwerk's ultimate, Hookshot.\n",
        "HP \n",
        "Health/hit points, referring to the health a unit has.\n",
        "inc \n",
        "Abbreviation for \"incoming\", usually referring to enemy players heading to a lane to gank.\n",
        "Initiation \n",
        "The act of starting (initiating) a teamfight. A good initiation can catch the enemy team off-guard, possibly losing quickly to the attacking team. A hero adept at performing this is called an initiator.\n",
        "Janggo \n",
        "Drums of Endurance's former name in WC3 DotA.\n",
        "Juking \n",
        "Running and weaving around trees, fog and other obstacles in such a manner to avoid and possibly escape an enemy.\n",
        "Jungling \n",
        "The process of killing the neutral creeps in the woods (aka \"jungle\") between the lanes. Killing ancient creeps also counts as jungling, but is more often referred to as ancienting or \"clearing ancients\". A hero adept at jungling (be it by having summons to absorb the creeps' attacks or having good durability/self-sustain) is termed a jungler.\n",
        "K - O\n",
        "Kick \n",
        "More commonly used to refer to Earth Spirit's Boulder Smash. May also refer to removing a player from a lobby or game or Tusk's Walrus Kick.\n",
        "Kite or Kiting \n",
        "See pulling. A technique where a hero gets the attention of a hostile unit to draw them away or force them to follow. Can also be used to refer to a period of repeated hit-and-run attacks where the target is kept out of range.\n",
        "KotL \n",
        "Initialism for Keeper of the Light.\n",
        "KS \n",
        "Either means \"kill steal\" or, usually jokingly, \"kill secure\". Firstly, it refers to the act of one hero stealing the kill of another hero that could have easily taken it themselves. Secondly, it refers to when another hero \"secures\" the kill if the carry or another hero could not do so themselves. The latter meaning is also sometimes interchangeably used with the former meaning in a satirical or comedic way.\n",
        "LC \n",
        "Initialism for Legion Commander.\n",
        "Leoric \n",
        "Wraith King's name in WC3 Dota.\n",
        "Long lane \n",
        "See Hard Lane.\n",
        "Lothar's or Lothar's Edge \n",
        "Shadow Blade's name in WC3 DotA.\n",
        "LS \n",
        "Initialism for Lifestealer.\n",
        "LSA \n",
        "Initialism for Lina's ability, Light Strike Array.\n",
        "Manfight \n",
        "Refers to when two heroes, usually melee, continuously attack one another without kiting (see above) until one hero either dies or retreats. Often implies a 1v1 battle without allies of either hero nearby. Heroes such as Ursa and Troll Warlord excel at manfights.\n",
        "Meatball \n",
        "Refers to Invoker's Chaos Meteor.\n",
        "Micro \n",
        "Micromanagement. Refers to (effective) control and usage of multiple units, items, and abilities in quick succession. Bad micromanagement can result in the many units becoming a hindrance or a liability rather than an asset or ability combos being wasted and/or used in the wrong situations, while a good one can easily overpower an enemy in seconds. Meepo, Invoker, and Chen are good examples of heroes who require good \"micro\".\n",
        "Mid/Middle \n",
        "The middle of the three lanes in the map.\n",
        "Miss, Missing, or MIA \n",
        "Mentions that a particular hero has gone absent (missing) from their lane, and is probably setting up for a gank. Warning missing enemy heroes is crucial to warn your allies of a possible ambush in any of the lanes. \"MIA\" is an acronym for \"Missing In Action\".\n",
        "MK \n",
        "Initialism for Monkey King.\n",
        "MKB \n",
        "Initialism for Monkey King Bar.\n",
        "MP \n",
        "Mana points, referring to the amount of mana a unit has.\n",
        "MS \n",
        "Initialism for Movement Speed, or sometimes \"missing\".\n",
        "Mute \n",
        "Refers either to the effect that prevents item usage, or blocking any means of communication from a player in the game to avoid harassment.\n",
        "N'aix Bomb \n",
        "Refers to when Lifestealer uses his ultimate, Infest, on an allied hero, preferably one with high mobility or with a Blink Dagger (and perhaps a disable), so that Lifestealer bursts out of his ally upon arrival, dealing additional damage and having two heroes down their target.\n",
        "Necro \n",
        "May refer to Necrophos or the Necronomicon. Rarely used to refer to Visage, the Bound Form of Necro'lic.\n",
        "Nerubian/Nerub \n",
        "Refers to Nyx Assassin and Weaver's former titles in WC3 DotA (the Nerubian Assassin and Nerubian Weaver, respectively).\n",
        "nj \n",
        "\"Nice job\", analogous to gj.\n",
        "NP \n",
        "Initialism for Nature's Prophet or \"no problem\".\n",
        "Nuke \n",
        "A spell whose purpose is to deal a large amount of damage immediately or in a very short span of time. Heroes adept at nuking are referred to as nukers.\n",
        "OC \n",
        "Initialism for Octarine Core.\n",
        "OD \n",
        "Initialism for Outworld Destroyer.\n",
        "Offlane \n",
        "See Hard Lane.\n",
        "Offlaner \n",
        "A hero sent down to the offlane/hard lane, usually on their own. These heroes usually have good escape abilities or are naturally durable to withstand being alone and facing usually two enemy heroes (and being more vulnerable to ganks).\n",
        "Omni \n",
        "Refers to Omniknight or Juggernaut's ultimate, Omnislash.\n",
        "OoM \n",
        "Initialism for \"out of mana\".\n",
        "OoV \n",
        "Initialism for Orb of Venom.\n",
        "Orb effect \n",
        "The name used for unique attack modifiers in WC3 DotA.\n",
        "Orb walk \n",
        "Using your orb effect (aka autocast) ability, i.e. Frost Arrows, Searing Arrows, to harass the enemy while moving forward to gain distance on the target and also preventing enemy creep aggro.\n",
        "P - T\n",
        "P \n",
        "Abbreviation for \"push\" or \"pause\". Players saying \"p\" means they are either suggesting the team to push an enemy lane or asking for a pause.\n",
        "PA \n",
        "Initialism for Phantom Assassin.\n",
        "Panda/Pandaren \n",
        "Brewmaster's former title in WC3 DotA.\n",
        "Pet \n",
        "A creature that a hero can summon or convert to their side. For example, the Eidolons are Enigma's pets. Could also refer to the cosmetic pets that follow your hero in-game.\n",
        "Pit Lord \n",
        "Underlord's former title in WC3 DotA.\n",
        "PL \n",
        "Initialism for Phantom Lancer.\n",
        "PP \n",
        "Initialism for \"pause please\".\n",
        "PotM \n",
        "Initialism for Mirana, the Princess of the Moon.\n",
        "Proc \n",
        "Short for \"Programmed Random Occurence\". It refers to the triggering of effects, whether the occurence is random (such as Wraith King's critical strikes on attack) or regular (such as Bristleback's quill spray every 250 damage he takes). When these effects trigger or \"process\", they are said to \"proc\". The rate of occurence for random effects is termed the \"Proc rate\".\n",
        "Pull or Pulling\n",
        "A technique where a hero gets the attention of a hostile unit to draw them away or force them to follow. This more usually refers to \"creep pulling\", which involves pulling lane creeps away from their lane by aggroing nearby creep camps into the chosen lane, which attracts the lane creeps and forces them to fight the aggroed creep camp for some time, which in turn makes the enemy creeps push forward, closer to the player's tower, letting them (or their carry) gain gold and experience underneath the safety of their tower.\n",
        "QoP \n",
        "Initialism for Queen of Pain.\n",
        "Rat \n",
        "A strategy in which heroes avoid 5-man teamfights, and focus on pushing other lanes instead.\n",
        "Rax \n",
        "Abbreviation for barracks.\n",
        "Recrow \n",
        "To reuse the courier after it has finished performing one of its deliveries/tasks. Since the courier can only perform one task for one player at a time, this is mentioned to remind/alert other players that the courier is now free to use.\n",
        "Reuse \n",
        "See recrow, usually mentioned by Southeast Asian players.\n",
        "Ricer \n",
        "Hardcore farmer whose main goal is to be extremely strengthened by the time they come out of their farming.\n",
        "Ring or RoB/RoH/RoR/RoT \n",
        "Refers to Ring of Basilius, Ring of Health, Ring of Regeneration, and Ring of Tarrasque respectively. \"Ring\" usually refers to one of these, based on context.\n",
        "RNG \n",
        "Random Number Generation, referring to the proc chance of abilities and items such as Bash or Evasion.\n",
        "Ro3 \n",
        "See Roshan, usually mentioned by Chinese players.\n",
        "Roamer \n",
        "A hero that jumps between lanes, especially in the early game, in order to gank enemy heroes or defend allies.\n",
        "Rock \n",
        "May refer to Warlock's ultimate: Chaotic Offering, Earth Spirit's Stone Remnants, the Ancient Rock and Granite Golems, or the Mud Golem's ability: Hurl Boulder.\n",
        "Rosh or RS \n",
        "Roshan is a difficult-to-kill neutral creep that drops the Aegis of the Immortal when killed (or Refresher Shard on his third death and afterward). Spawns at the beginning of the game, respawns 8-11 minutes after he is killed.\n",
        "RP \n",
        "Magnus' ultimate, Reverse Polarity.\n",
        "Safe lane \n",
        "The lane of either faction, where the creep wave meets closest to the tower, making farming easier and less risky for those in the lane (bottom lane for Radiant, top lane for Dire). See Lane.\n",
        "SB \n",
        "Initialism for Spirit Breaker or Shadow Blade.\n",
        "Scepter \n",
        "Mostly refers to Aghanim's Scepter, although can also refer to Eul's Scepter of Divinity or Ghost Scepter.\n",
        "SD \n",
        "Initialism for Shadow Demon or \"self denial\" (see suicide).\n",
        "SF \n",
        "Initialism for Shadow Fiend.\n",
        "Sheepstick \n",
        "Refers to the Scythe of Vyse, since back in WC3 DotA, it actually turned the target into a sheep (unlike in Dota 2, where the hex model is a pig).\n",
        "Short lane\n",
        "See Safe Lane.\n",
        "Silence \n",
        "The effect of preventing a unit from casting spells, but can still allows item usage and passive abilities are unaffected. See the main article for more information.\n",
        "SK \n",
        "Initialism for Sand King. May also refer to Wraith King in reference to his former title, Skeleton King, or Terrorblade, in reference to his former title Soul Keeper.\n",
        "Skillshot \n",
        "An ability that requires proper aim and timing to hit an enemy. Some examples would be Sacred Arrow and Powershot.\n",
        "Smoke \n",
        "Refers to either Smoke of Deceit, or Riki's ability Smoke Screen.\n",
        "Snowballing \n",
        "The situation where a hero or team just gets stronger as the games goes on (usually through getting kills) to the point where it is very difficult to stop them, much like a snowball rolling down a hill getting bigger.\n",
        "SnY \n",
        "Initialism for Sange and Yasha.\n",
        "SOD \n",
        "Initialism for Smoke of Deceit.\n",
        "Solo \n",
        "Being the only hero on the lane. A player may call for \"solo\" to prevent other players from interrupting their lane and splitting experience and gold.\n",
        "Soul Keeper \n",
        "Terrorblade's former title in WC3 DotA.\n",
        "Spirit Hero \n",
        "Shorthand for Ember Spirit, Storm Spirit, and Void Spirit. These three heroes share many attributes, such as typically being played mid, and having versatile kits featuring heavy burst damage and extremely reliable long-range engagement/escape tools. Earth Spirit is generally not included in this definition, as he is more often played as a support.\n",
        "Squishy \n",
        "A hero that can only take relatively little damage or abuse before dying.\n",
        "SR \n",
        "Initialism for status resistance.\n",
        "SS\n",
        "See missing. Mentions that a particular hero has gone missing and is probably setting up for a gank, e.g. \"Earthshaker ss\" means that Earthshaker isn't visible to the team. Also refers to the ultimate of a hero (\"special/super skill\"), usually mentioned by Southeast Asian players. May also refer to Storm Spirit, Shadow Shaman or Sun Strike.\n",
        "Stacking\n",
        "May also be referred to as \"creep stacking\", it is a technique where a creep camp is aggroed at a certain time before neutral spawning, pulling them away from their camp's area and allowing a new set of creeps to spawn. This can be done multiple times, usually up to three at maximum, then letting their team's carry farm the stacked creeps for a massive gold and experience gain.\n",
        "Static Farming\n",
        "A sub-type of farming where you aim to only kill enemy lane creeps when they have low health and deny the allied lane creeps whenever possible, with the goal of maintaining an equilibrium of the lane creeps in a safe position. This is often a slower method of farming, but allows you to farm safely with little risk of dying to a gank.\n",
        "Stygian\n",
        "References the Desolator's former full name in WC3 DotA.\n",
        "Suicide\n",
        "The act of killing oneself to deny the enemy of gold and experience. May also directly refer to Techies' ability Blast Off! (formerly Suicide Squad, Attack!).\n",
        "Suicide lane\n",
        "See Hard Lane.\n",
        "Summoner\n",
        "A hero whose primary function is summoning or controlling creeps.\n",
        "Support\n",
        "A hero whose primary function is to help their own team through heals, buffs, and detection, or sabotaging the enemy team through disables, slows, or debuffs.\n",
        "TA \n",
        "Initialism for Templar Assassin.\n",
        "Tank \n",
        "See Durable.\n",
        "Tauren Chieftain (or simply Tauren) \n",
        "Elder Titan's former title in WC3 DotA.\n",
        "TB \n",
        "Initialism for Terrorblade.\n",
        "Teamwipe \n",
        "The act of an entire team dying at once.\n",
        "THD \n",
        "Twin Head Dragon, the title of Jakiro.\n",
        "Throw \n",
        "The act of giving a supreme advantage to the supposedly losing team, letting them win the match instead. This can be intentional or unintentional.\n",
        "Throne \n",
        "See Ancient. Remnant of WC3 DOTA in which the Scourge's (Dire) Ancient is using the Frozen Throne model.\n",
        "Top \n",
        "Refers to the top lane of the map. For the Radiant, this is the lane to the North. For the Dire, this is the lane to the West.\n",
        "TP \n",
        "Initialism for Teleport or Town Portal Scroll.\n",
        "True Sight \n",
        "Anything invisible caught in the radius of a True Sight source gets revealed. Towers, a Gem of True Sight, Sentry Wards, Dust of Appearance, and some abilities of certain heroes (such as Slardar's Amplify Damage and Zeus' Lightning Bolt) all provide True Sight. See the main article for more information.\n",
        "Turnaround \n",
        "The act of a team facing the enemy in a situation where they might be expected to run, such as when a team initiates on the other. Instead of running away, they try to turn the battle around and fight back, possibly even winning the battle themselves.\n",
        "U - Z\n",
        "UAM \n",
        "Initialism for unique attack modifier.\n",
        "Ult or Ulti\n",
        "Abbreviation for Ultimate.\n",
        "Veno \n",
        "Abbreviation for Venomancer. Rarely used to refer to Orb of Venom.\n",
        "VS \n",
        "Initialism for Vengeful Spirit or Void Spirit. Less commonly used to mean \"versus\".\n",
        "Wards\n",
        "Items that can be placed almost anywhere on the map and provide vision around the location for a few minutes. May also refer to the unit type of the same name.\n",
        "WD \n",
        "Initialism for Witch Doctor.\n",
        "Well \n",
        "The fountain from either team. Derived from its name in WC3 DotA, \"Well of Eternity\".\n",
        "Wipe \n",
        "See teamwipe.\n",
        "WK \n",
        "Initialism for Wraith King.\n",
        "wp \n",
        "\"Well Played\". Usually said by either teams when they are winning or losing and the match is either about to end or when a player did something skillful. Usually paired up with \"gg\".\n",
        "WR \n",
        "Initialism for Windranger.\n",
        "WW \n",
        "Initialism for Winter Wyvern. It also may refer to \"Wind Walk\" abilities, which is derived from the common DotA name of the invisibility spells of Bounty Hunter, Clinkz, Shadow Blade and Storm, and the invisibility spells of Nyx Assassin and Invoker (which already had unique names in DotA). All 6 of them are based on the Warcraft 3 spell named \"Wind Walk\".\n",
        "Zoning\n",
        "The act of harassing an enemy hero away from the creep zone in order to prevent them from gaining experience.\n",
        "\"\"\"\n",
        "\n",
        "# reference : Dota 2 wiki,https://dota2.fandom.com/wiki/Glossary"
      ],
      "metadata": {
        "id": "hzj1QWKRQXOy"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dota_corpus = corpus_dota.split(\" \")\n",
        "\n",
        "import re\n",
        "\n",
        "dota_text =[]\n",
        "for i in dota_corpus:\n",
        "  dota = re.sub(r\"[^a-z0-9]+\", \"\", i.lower())\n",
        "  dota_text.append(dota)\n",
        "\n",
        "dota_cor = []\n",
        "for i in dota_text:\n",
        " a = i.split(\",\")\n",
        " dota_cor.append(a)\n",
        "\n",
        "#reference: comp5046,lab02,https://colab.research.google.com/drive/1-l7gzLZ71ERuJ_ktG1nKTU6G8UuEvseN?usp=sharing\n",
        "#reference: comp5046,lab05, https://colab.research.google.com/drive/1zO2FYT9J61Lj_b5S_tMH2Uo_FhjK0u18?usp=sharing"
      ],
      "metadata": {
        "id": "qfRaCZjGSHr1"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the matrix by dota corpus\n",
        "word_emb_model_domain=Word2Vec(sentences=dota_cor,size=100, \n",
        "                 window=3,\n",
        "                 min_count=1, #count in word2vec of word appear at minimum once\n",
        "                 workers=4,sg=1)\n",
        "#reference: comp5046,lab02,https://colab.research.google.com/drive/1-l7gzLZ71ERuJ_ktG1nKTU6G8UuEvseN?usp=sharing"
      ],
      "metadata": {
        "id": "DoWWxsZzWgfe"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM=141\n",
        "\n",
        "embedding_matrix=[] \n",
        "for word in word_list:\n",
        "    try:\n",
        "        word_embed=list(word_emb_model_domain.wv[word])\n",
        "        word_embed.extend(list(word_2_pos[word]))\n",
        "        word_embed.extend(list(word_2_parse[word]))\n",
        "        word_embed.extend([len(word)])\n",
        "        embedding_matrix.append(word_embed)\n",
        "        \n",
        "    except:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "embedding_matrix=np.array(embedding_matrix)\n",
        "embedding_matrix.shape\n",
        "\n",
        "#reference: comp5046,lab02,https://colab.research.google.com/drive/1-l7gzLZ71ERuJ_ktG1nKTU6G8UuEvseN?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy1jAvoqXG7N",
        "outputId": "4d52fd44-8353-43a6-fe8c-fdc2e49599e8"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11243, 141)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6rVmnd6_cd8"
      },
      "source": [
        "###3.Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 baseline model"
      ],
      "metadata": {
        "id": "Ox_PJ2fCqkc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "#help the model to more readable \n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "id": "Vi3M5JxBkD4f"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM_CRF_base(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF_base, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space. \n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "id": "ysgq2ZtEpQC2"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1.1 Training for baseline model\n",
        "\n"
      ],
      "metadata": {
        "id": "80Kj9O846kWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "import numpy as np\n",
        "def cal_acc_base(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    f1score=f1_score(ground_truth,predicted,average=\"micro\")\n",
        "    return predicted,ground_truth, f1score\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "id": "X5aeRQOYidvT"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concate all input and output index\n",
        "train_data=train_input_index+val_input_index\n",
        "train_label=train_output_index+val_output_index"
      ],
      "metadata": {
        "id": "CwgErmVxi8mH"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up baseline model \n",
        "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\") #put all in cpu\n",
        "HIDDEN_DIM = 70\n",
        "\n",
        "base_model = BiLSTM_CRF_base(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)#base model\n",
        "optimizer = optim.SGD(base_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "id": "pVxw6wTCpoYK"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the baseline model\n",
        "\n",
        "import datetime\n",
        "\n",
        "print(\"=\"*115)\n",
        "print(\"                                   Baseline model training on Validation                          \")\n",
        "print(\"=\"*115)\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    base_model.train()#base model\n",
        "    for i, idxs in enumerate(train_data):\n",
        "        tags_index = train_label[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        base_model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = base_model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    base_model.eval()\n",
        "    # Call the cal_acc functions you implemented as required\n",
        "    _, _, train_acc = cal_acc_base(base_model,train_input_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc_base(base_model,val_input_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = base_model.neg_log_likelihood(sentence_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train f1-score: %.4f, val loss: %.2f, val f1-score: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "    #reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxNcOmYlqGEW",
        "outputId": "47d99f7a-ca25-4186-ecd9-16d5db0d1990"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================================================================================\n",
            "                                   Baseline model training on Validation                          \n",
            "===================================================================================================================\n",
            "Epoch:1, Training loss: 35806.00, train f1-score: 0.9939, val loss: 709.27, val f1-score: 0.9962, time: 577.34s\n",
            "Epoch:2, Training loss: 2272.02, train f1-score: 0.9989, val loss: 150.67, val f1-score: 0.9995, time: 585.80s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 model designed"
      ],
      "metadata": {
        "id": "7shD7mT9qc1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "W7VeLxRd_fqS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim , num_layers, use_crf, attention_method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        #hyper-parameter\n",
        "        self.num_layers = num_layers\n",
        "        self.attention_method = attention_method\n",
        "        self.use_crf = use_crf\n",
        "\n",
        "        self.general_attention_weight = nn.parameter.Parameter(torch.Tensor(1,self.hidden_dim,self.hidden_dim),requires_grad = True)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True) #bi-direction\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size) if not attention_method else nn.Linear(hidden_dim*2,self.tagset_size)\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG],:] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "\n",
        "        #attention setting\n",
        "        if self.attention_method:\n",
        "          lstm_out = torch.squeeze(lstm_out,1)\n",
        "          left_self = lstm_out.view(1,lstm_out.size(0),lstm_out.size(1))\n",
        "          right_self = left_self.view(left_self.size(0),left_self.size(2),left_self.size(1))\n",
        "\n",
        "          if \"scale\" in self.attention_method.lower():\n",
        "            weight_att = nn.functional.softmax(torch.bmm(left_self,right_self)/np.sqrt(self.hidden_dim),dim =-1)\n",
        "          elif\"general\" in self.attention_method.lower():\n",
        "            step_one = torch.bmm(left_self,self.general_attention_weight)\n",
        "            step_two = torch.bmm(step_one,right_self)\n",
        "            weight_att = nn.functional.softmax(step_two,dim=-1)\n",
        "          elif\"dot product\"in self.attention_method.lower():\n",
        "            weight_att = nn.functional.softmax(torch.bmm(left_self,right_self),dim=-1)\n",
        "          \n",
        "          output = torch.bmm(weight_att,left_self)\n",
        "          concat_output = torch.cat((output,left_self),dim = -1)\n",
        "          lstm_out = concat_output.view(len(sentence), self.hidden_dim*2)\n",
        "        else:\n",
        "          lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "\n",
        "\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        if self.use_crf==True: #using crf\n",
        "           lstm_feats=self._get_lstm_features(sentence)\n",
        "           score,tag_=self._viterbi_decode(lstm_feats)\n",
        "           return score,tag_\n",
        "        else:           #no crf\n",
        "           lstm_feats = self._get_lstm_features(sentence)\n",
        "           return lstm_feats,torch.argmax(lstm_feats,-1)\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2.1 Training model with hyper-parameters"
      ],
      "metadata": {
        "id": "hvAa8MYoy6rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "\n",
        "def cal_acc(model, input_index, output_index,use_crf = True):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for x,y in zip(input_index,output_index):\n",
        "        input_tensor=torch.tensor(x).to(device)\n",
        "        _,output=model(input_tensor)\n",
        "        ground_truth.extend(y)\n",
        "        \n",
        "        if use_crf==True:\n",
        "            predicted.extend(output)\n",
        "        else:\n",
        "            predicted.extend(list(output.cpu().numpy()))\n",
        "    f1= torch.tensor(f1_score(ground_truth,predicted,average=\"micro\")).to(device)\n",
        "    return ground_truth, predicted, f1\n",
        "    #reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "id": "x3x3dFrGd5NO"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a list of configs\n",
        "#hyper-parameter\n",
        "configs = [{ \"num_layers\":1,\"use_crf\":False,\"attention_method\":\"general\"\n",
        "    },\n",
        "    { \"num_layers\":1, \"use_crf\":False ,\"attention_method\":\"scale\"\n",
        "    },\n",
        "    { \"num_layers\":1, \"use_crf\":False ,\"attention_method\":\"dot product\"     \n",
        "    },\n",
        "    { \"num_layers\":2, \"use_crf\":False,\"attention_method\":\"general\"  \n",
        "    },\n",
        "    { \"num_layers\":2, \"use_crf\":False,\"attention_method\":\"scale\"   \n",
        "    },\n",
        "    { \"num_layers\":2, \"use_crf\":False,\"attention_method\":\"dot product\"     \n",
        "    },\n",
        "    { \"num_layers\":1,\"use_crf\":True ,\"attention_method\":\"general\"       \n",
        "    },\n",
        "    { \"num_layers\":1,\"use_crf\":True ,\"attention_method\":\"scale\"\n",
        "    },\n",
        "    { \"num_layers\":1, \"use_crf\":True ,\"attention_method\":\"dot product\"\n",
        "    },\n",
        "    { \"num_layers\":2, \"use_crf\":True ,\"attention_method\":\"general\"\n",
        "    },\n",
        "    { \"num_layers\":2, \"use_crf\":True ,\"attention_method\":\"scale\"\n",
        "    },\n",
        "    { \"num_layers\":2, \"use_crf\":True ,\"attention_method\":\"dot product\"\n",
        "    }]\n",
        "\n",
        "# reference: comp5046,lab11, https://colab.research.google.com/drive/1c4NnDzNmzVJmXolwWdRO1tRhKX6aROWf?usp=sharing"
      ],
      "metadata": {
        "id": "ya9fXtwaisx9"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from re import A\n",
        "device=torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")#put all in cpu\n",
        " \n",
        "for config in configs:\n",
        "    HIDDEN_DIM=70\n",
        "    vocab_size = len(word_to_ix) # the size of vocabulary\n",
        "    use_crf=True\n",
        "    model = BiLSTM_CRF(vocab_size,tag_to_ix, EMBEDDING_DIM,HIDDEN_DIM,config[\"num_layers\"], config[\"use_crf\"], config[\"attention_method\"]).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.015,weight_decay=1e-4)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "  \n",
        "\n",
        "    print(\"=\"*111)\n",
        "    print(\"          .    number of layers = {}    &    use_crf = {}    &    attention_method = {}    . \".format(config[\"num_layers\"], config[\"use_crf\"], config[\"attention_method\"]))\n",
        "    print(\"=\"*111)\n",
        "\n",
        "\n",
        "    for epoch in range(2):\n",
        "      time1 = datetime.datetime.now()\n",
        "      train_loss = 0\n",
        "      model.train()\n",
        "      for i, idxs in enumerate(train_data):\n",
        "        tags_index = train_label[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        \n",
        "        if use_crf:\n",
        "          loss=model.neg_log_likelihood(sentence_in,targets)\n",
        "        else:\n",
        "          lstm_feats,tags=model(sentence_in)\n",
        "          loss=loss_func(lstm_feats,targets)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "      \n",
        "      \n",
        "      max_val_acc=0\n",
        "      model.eval() \n",
        "      # Call the cal_acc functions you implemented as required\n",
        "      _, _, train_acc = cal_acc(model,train_input_index,train_output_index,use_crf=use_crf)\n",
        "      _, _, val_acc = cal_acc(model,val_input_index,val_output_index,use_crf=use_crf)\n",
        "\n",
        "      if(val_acc > max_val_acc):\n",
        "        best_model=model\n",
        "        max_val_acc=val_acc  \n",
        "      \n",
        "      if  use_crf:\n",
        "        val_loss=0\n",
        "        for i, idxs in enumerate(val_input_index):\n",
        "          tags_index = val_output_index[i]\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "          loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "          val_loss+=loss.item()\n",
        "        time2 = datetime.datetime.now()\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train f1 score: %.4f, val loss: %.2f, val f1 score: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "        \n",
        "      else:\n",
        "        time2 = datetime.datetime.now()\n",
        "        print(\"Epoch:%d, Training loss: %.2f, train f1 score: %.4f, val f1 score: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLvjj-_c9IqD",
        "outputId": "4e91dea6-604b-459c-90a4-00640a1afb8d"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================================================================\n",
            "          .    number of layers = 1    &    use_crf = False    &    attention_method = general    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28911.86, train f1 score: 0.9959, val loss: 470.55, val f1 score: 0.9975, time: 558.59s\n",
            "Epoch:2, Training loss: 1961.12, train f1 score: 0.9994, val loss: 70.02, val f1 score: 0.9997, time: 543.27s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 1    &    use_crf = False    &    attention_method = scale    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28095.83, train f1 score: 0.9957, val loss: 399.62, val f1 score: 0.9976, time: 544.38s\n",
            "Epoch:2, Training loss: 1734.59, train f1 score: 0.9995, val loss: 62.63, val f1 score: 0.9997, time: 458.83s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 1    &    use_crf = False    &    attention_method = dot product    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28439.49, train f1 score: 0.9958, val loss: 399.63, val f1 score: 0.9978, time: 542.22s\n",
            "Epoch:2, Training loss: 1700.59, train f1 score: 0.9994, val loss: 65.40, val f1 score: 0.9996, time: 456.36s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 2    &    use_crf = False    &    attention_method = general    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28458.52, train f1 score: 0.9958, val loss: 399.25, val f1 score: 0.9974, time: 546.10s\n",
            "Epoch:2, Training loss: 1921.86, train f1 score: 0.9992, val loss: 71.17, val f1 score: 0.9996, time: 459.96s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 2    &    use_crf = False    &    attention_method = scale    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28946.25, train f1 score: 0.9963, val loss: 375.76, val f1 score: 0.9983, time: 547.20s\n",
            "Epoch:2, Training loss: 1698.84, train f1 score: 0.9997, val loss: 61.62, val f1 score: 0.9998, time: 460.80s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 2    &    use_crf = False    &    attention_method = dot product    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28224.42, train f1 score: 0.9929, val loss: 440.54, val f1 score: 0.9954, time: 541.67s\n",
            "Epoch:2, Training loss: 1891.50, train f1 score: 0.9987, val loss: 110.13, val f1 score: 0.9992, time: 457.55s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 1    &    use_crf = True    &    attention_method = general    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28718.69, train f1 score: 0.9956, val loss: 441.96, val f1 score: 0.9976, time: 584.95s\n",
            "Epoch:2, Training loss: 2053.10, train f1 score: 0.9995, val loss: 80.32, val f1 score: 0.9997, time: 500.04s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 1    &    use_crf = True    &    attention_method = scale    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28852.37, train f1 score: 0.9952, val loss: 430.52, val f1 score: 0.9974, time: 584.40s\n",
            "Epoch:2, Training loss: 1967.00, train f1 score: 0.9994, val loss: 56.18, val f1 score: 0.9997, time: 498.32s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 1    &    use_crf = True    &    attention_method = dot product    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28408.53, train f1 score: 0.9964, val loss: 435.55, val f1 score: 0.9979, time: 580.88s\n",
            "Epoch:2, Training loss: 1801.28, train f1 score: 0.9994, val loss: 82.66, val f1 score: 0.9996, time: 493.07s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 2    &    use_crf = True    &    attention_method = general    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28981.35, train f1 score: 0.9955, val loss: 471.99, val f1 score: 0.9973, time: 586.83s\n",
            "Epoch:2, Training loss: 1885.00, train f1 score: 0.9994, val loss: 67.08, val f1 score: 0.9996, time: 501.62s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 2    &    use_crf = True    &    attention_method = scale    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 28138.12, train f1 score: 0.9962, val loss: 375.35, val f1 score: 0.9983, time: 583.29s\n",
            "Epoch:2, Training loss: 1760.01, train f1 score: 0.9996, val loss: 60.37, val f1 score: 0.9998, time: 497.12s\n",
            "===============================================================================================================\n",
            "          .    number of layers = 2    &    use_crf = True    &    attention_method = dot product    . \n",
            "===============================================================================================================\n",
            "Epoch:1, Training loss: 27965.16, train f1 score: 0.9966, val loss: 370.40, val f1 score: 0.9982, time: 583.33s\n",
            "Epoch:2, Training loss: 1683.29, train f1 score: 0.9996, val loss: 60.20, val f1 score: 0.9997, time: 495.98s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluation Setup"
      ],
      "metadata": {
        "id": "xQj0DbJOzWdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model design\n",
        "y_true,y_pred,_=cal_acc(best_model,val_input_index,val_output_index,use_crf)\n",
        "\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "# generate classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk-8IbRJ0zku",
        "outputId": "e2c61602-f3e9-4ef3-f1b7-d6a2680a59de"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9994    1.0000    0.9997      1641\n",
            "           D     0.9975    0.9899    0.9937       398\n",
            "           O     1.0000    1.0000    1.0000     18985\n",
            "           P     0.9995    1.0000    0.9997      3936\n",
            "           S     1.0000    0.9997    0.9998      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9986    0.9993    0.9990      1469\n",
            "\n",
            "    accuracy                         0.9998     33354\n",
            "   macro avg     0.9993    0.9984    0.9989     33354\n",
            "weighted avg     0.9998    0.9998    0.9998     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model\n",
        "y_true,y_pred,_=cal_acc(base_model,val_input_index,val_output_index,use_crf)\n",
        "\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "# generate classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuby-JXegEGH",
        "outputId": "ca8f7645-3b49-47ca-e91b-a618c8a4b9a5"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9994    0.9976    0.9985      1641\n",
            "           D     0.9822    0.9724    0.9773       398\n",
            "           O     0.9998    1.0000    0.9999     18985\n",
            "           P     1.0000    1.0000    1.0000      3936\n",
            "           S     0.9970    1.0000    0.9985      3322\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9979    0.9932    0.9956      1469\n",
            "\n",
            "    accuracy                         0.9993     33354\n",
            "   macro avg     0.9966    0.9947    0.9957     33354\n",
            "weighted avg     0.9992    0.9993    0.9992     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Performance Comparison"
      ],
      "metadata": {
        "id": "yHxSkP26zoVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "performance = {\n",
        "    'Model': ['Baseline', 'Model_designed'], #column\n",
        "    'T-F1': ['0.9992', '0.9998'],\n",
        "    'T-F1(T)': ['0.9956', '0.9999'],\n",
        "    'T-F1(S)': ['0.9985', '0.9998'],\n",
        "    'T-F1(C)': ['0.9985', '0.9997'],\n",
        "    'T-F1(D)': ['0.9773', '0.9937'],\n",
        "    'T-F1(P)': ['1.0000', '0.9997'],\n",
        "    'T-F1(O)': ['0.9999', '1.0000'] }\n",
        "df = pd.DataFrame(data=performance)\n",
        "print(\"=\"*80)\n",
        "print(\" \"*35, \"Metrics\")\n",
        "print('-'*80)\n",
        "print(df)\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNvO3Pqp0Wzp",
        "outputId": "ec85ef46-f0ae-4703-f2d4-dbf7b2c035d0"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                    Metrics\n",
            "--------------------------------------------------------------------------------\n",
            "            Model    T-F1 T-F1(T) T-F1(S) T-F1(C) T-F1(D) T-F1(P) T-F1(O)\n",
            "0        Baseline  0.9992  0.9956  0.9985  0.9985  0.9773  1.0000  0.9999\n",
            "1  Model_designed  0.9998  0.9999  0.9998  0.9997  0.9937  0.9997  1.0000\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ablation Study - different input embedding model"
      ],
      "metadata": {
        "id": "889lzYHYzoNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "performance = {\n",
        "    'Baseline_model': ['POS + Parses + word embedding'], #column\n",
        "    ' |   0.9957': ['|   0.9989'] }\n",
        "df = pd.DataFrame(data=performance)\n",
        "print(\"=\"*60)\n",
        "print(\" \"*12, \"Model\",\" \"*17,\"F1 mean score\")\n",
        "print('-'*60)\n",
        "print(df)\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjjJ2_HZ0XXw",
        "outputId": "673691d9-e4aa-44aa-d33b-d68765dcca60"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "             Model                   F1 mean score\n",
            "------------------------------------------------------------\n",
            "                  Baseline_model  |   0.9957\n",
            "0  POS + Parses + word embedding  |   0.9989\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ablation Study - different attention strategy"
      ],
      "metadata": {
        "id": "9OArm1eMzn6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "performance = {\n",
        "    'No attention method': ['scale', 'general','dot product'], #column\n",
        "    '      0.9957': ['      0.99975', '      0.99965','      0.999525'], }\n",
        "df = pd.DataFrame(data=performance)\n",
        "print(\"=\"*50)\n",
        "print(\" \"*5, \"Attention method\",\" \"*5,\"F1 mean score\")\n",
        "print('-'*50)\n",
        "print(df)\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JREpBaQC0X3L",
        "outputId": "b55b9265-fd71-4f42-8438-8c987a12677c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "      Attention method       F1 mean score\n",
            "--------------------------------------------------\n",
            "  No attention method          0.9957\n",
            "0               scale         0.99975\n",
            "1             general         0.99965\n",
            "2         dot product        0.999525\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Ablation Study - different Stacked layer or # of encoder/decoder strategy\n"
      ],
      "metadata": {
        "id": "ZuZPw_rPz7aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "performance = {\n",
        "    'Baseline model': ['scale', 'scale', 'general','general','dot product','dot product'], #column\n",
        "    '      layer = 1': ['     layer = 1', '     layer = 2','     layer = 1','     layer = 2','     layer = 1','     layer = 2'],\n",
        "    '    0.9957': ['   0.9997', '   0.9996','   0.9997','   0.9998','   0.9996','   0.99945']}\n",
        "df = pd.DataFrame(data=performance)\n",
        "print(\"=\"*50)\n",
        "print(\" \"*5, \"Model\",\" \"*10,\"strategy\",\" \"*5,\"F1 score\")\n",
        "print('-'*50)\n",
        "print(df)\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG0M2xb60YcF",
        "outputId": "7b499aad-da46-486f-a2fc-1e9e65d8c8a4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "      Model            strategy       F1 score\n",
            "--------------------------------------------------\n",
            "  Baseline model       layer = 1      0.9957\n",
            "0          scale       layer = 1      0.9997\n",
            "1          scale       layer = 2      0.9996\n",
            "2        general       layer = 1      0.9997\n",
            "3        general       layer = 2      0.9998\n",
            "4    dot product       layer = 1      0.9996\n",
            "5    dot product       layer = 2     0.99945\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Ablation Study - with/without CRF"
      ],
      "metadata": {
        "id": "0RxVgEdMz-Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "performance = {\n",
        "    'baseline without crf': ['model design without crf', 'model design with crf'], #column\n",
        "    '      0.9957': ['      0.9996', '      0.9996833'], }\n",
        "df = pd.DataFrame(data=performance)\n",
        "print(\"=\"*50)\n",
        "print(\" \"*5, \"strategy\",\" \"*15,\"F1 mean score\")\n",
        "print('-'*50)\n",
        "print(df)\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yLU1bXc0Y9U",
        "outputId": "bdaef001-7d8d-4acb-dac9-9c6981dbbcdb"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "      strategy                 F1 mean score\n",
            "--------------------------------------------------\n",
            "       baseline without crf           0.9957\n",
            "0  model design without crf           0.9996\n",
            "1     model design with crf        0.9996833\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Testing"
      ],
      "metadata": {
        "id": "JgXaEUc4qNlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save and load the best model\n",
        "torch.save(best_model,\"best_model.pt\") \n",
        "model = torch.load(\"best_model.pt\")\n",
        "#reference: comp5046,lab05, https://colab.research.google.com/drive/1zO2FYT9J61Lj_b5S_tMH2Uo_FhjK0u18?usp=sharing"
      ],
      "metadata": {
        "id": "i91RktQ2km6t"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model,input_index):\n",
        "    predicted=[]\n",
        "    for x in input_index:\n",
        "        input_tensor=torch.tensor(x).to(device)\n",
        "        _,output=model(input_tensor)   \n",
        "        for idx in output:           \n",
        "            predicted.append(ix_to_tag[idx])\n",
        "    return predicted  \n",
        "\n",
        "\n",
        "#reference: comp5046,lab09, https://colab.research.google.com/drive/1efZZFttmHKXHbQtNzAVHzjf8FUUD9cWR?usp=sharing"
      ],
      "metadata": {
        "id": "cf8oNWHCmISu"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=predict(best_model,test_input_index)"
      ],
      "metadata": {
        "id": "-H-s6MUcmIQw"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id = range(len(prediction))\n",
        "test_prediction={'ID':id,'Predicted':prediction}\n",
        "df=pd.DataFrame(test_prediction)\n",
        "df.to_csv('result.csv',index=False)"
      ],
      "metadata": {
        "id": "U_o0oEFpmINB"
      },
      "execution_count": 130,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "5046 ASM2.ipynb",
      "provenance": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}